# 統計・検定手法に悩んだ時のガイド
また統計や検定に馴染みがない人はこちらが役立ちます。[→統計・検定技術の基礎知識](https://ai-trend.jp/basic-study/#estimator)

## きっかけ
研究などで有意性を示すときにt検定をすることはよくよくあると思うのですが、なかなかそんな単純な2群の比較ばかりとは限りません。そこで大学の時の担当教員に[『すぐわかる統計処理の選び方』](https://ci.nii.ac.jp/ncid/BB03230711)がとてもわかりやすいという話を聞き、購入しました。

たしかに難解な数学的処理についてページを割くことなく「こういう状況にはこれ！」というようなカタログ的な書籍の作り方をしているので、いつも作業の横においておきたいなと思い、このページでまとめることにしました。なお、項目のまとめ方は個人的により分かりやすく全く書き換えています。

## データの下ごしらえ段階
まずデータを集めた場合には必ずこのステップは機械的・反射的に必ず処理します。

なぜならかき集めたデータはほとんどの場合標本の性質であり、知りたいのは母集団の性質であるからです。ここでやるべきことは

* ヒストグラムによるデータの分布の把握*▶データのばらつき具合の把握*
* 基礎統計量（最小・最大・平均・標準偏差・分散）の計算*▶データの特徴の把握*

です。これにより「このくらいの値が一般的（＝はずれ値ではない）んだなあ」などといった感覚的な部分を読み取ります。「感覚的」というとずいぶん曖昧なものだなあと思うかもしれませんが、まずは*見えていないものを自分の想像力の及ぶ範囲まど引っ張ってこないと次の対策が立ちません*。一方上記の手法でデータの特徴を読み取れればむしろラッキーで、世の中には被験者数の少ない実験や何度も起きないレアな現象ではヒストグラムにしてもデータの全体像がみえないことも多々あります。そのときに使うのが**母平均の区間推定・検定**で、データの姿がよくある形（**すなわち、正規分布**）であることなどを仮定してとりあえず次のステップに進むことにします。

## 「差」があるかどうかを調べる場合
「AとBには差がある」（＝データの平均のブレではなく、明確に違いがあると述べたい）というときに使います。


### 2つのデータ群がある場合
このあたりが統計の教科書で真っ先に出てくるt検定などの話のレベルです。一番シンプルな2群の比較です。2群の比較はつまり「**AとBに差はあるのか**」を示すときに使いますが、あまりにもベーシックすぎてなんでもこれで検定すればいいのではと誤解される方も多いです。実際には2群の検定で最も重要なのは**その2群はt検定がふさわしい**のか判別することです。計算はPCがやってくれますし。

データの全体像がわからない or 等分散性が保証できない2群はそもそも**ウェルチの検定**、同じ被験者の１回目のデータと２回目のデータの比較では2群に関連性があるので**対応のある場合の検定**を行う必要があります。

またそもそも、正規性が不安（中心極限定理を使えない等）ときにはそもそもt検定など行えませんからその他の**ウィルコクスンの順位和検定**や**コルモゴロフ・スミルノフの検定**を利用することになります。

### ３つ以上のデータ群がある場合
3つ以上となると「差があるか」を調べるだけでも群間の比較がが大変になります。

具体的には下のような分析手法にトライすることになりますが、そもそも多重比較の数的処理の方法にも何種類かあり、計算のしくみ上回避できない誤差があります。一般的に群の数が多ければ多いほど判定が甘くなる事が多いので慎重に行います。

* 分散分析*▶そもそもすべての群の平均の差はそもそも「ブレ」として許容できるのではないか*
* 多重比較*▶サンプリングによる「ブレ」でないとすると、どの群とどの群で差があるか*
* 繰り返しの測定など対応のあるデータ*▶フリードマンの検定*

まだ２つの比較同様、正規性に不安があればクラスカルウォリスの検定などを利用し、順位に置き換えることで**情報量を少し犠牲にして検定**します。

## 「因子」に基づく原因を探る場合
この場合はそもそもデータ自体が多変数（たとえば体重と身長の組み合わせのように１つのデータが複数からなる）の場合に利用する方法。

### 因子と因子の関係性を明らかにする
これまでは単変数の比較、つまりはマクロなデータのみかたをしていました。つまり被験者の体重、体脂肪、身長などの複数の項目の中から一つを取り出してグループ同士で比較していましたが、今度はそれらをもっとデータ自身の内側、「そもそも体重なんて身長と関連があるのだから。。」といったデータ自身を構成する因子について分析します。

* 共分散と相関係数の算出*▶因子同士の関係性の強さの把握*
* 単回帰分析*▶因子同士の関係性の数式化*
* 主成分分析*▶複数の因子を１つの因子に置き換える*

特に変数が増えて３変数以上になると、単回帰分析は重回帰分析になり、主成分分析も同様に３変数以上で行えます。回帰分析は**ある項目は他の項目で推測できる**分析に対して、主成分分析はそのデータの性質を明快に示す指標を得るために**複数の項目を１つにまとめた新たな説明変数を生み出す**分析手法であり、まったくことなるので注意が必要です。逆にデータ自体によりわかりやすく説明できる潜在的な因子を探るのは**因子分析**です。

### 因子を活用した推測・判別
説明の都合上、前節で単回帰分析を登場させています（実際に因子同士の関係性をみるのに回帰直線は便利）が、多くの研究の場合回帰する目的は**値の推測**のはずです。また因子はなにかの「原因」担っていることが多いですから総合的に見て「この因子とこの因子がこのくらいになると発生する」といった**判別分析**といった、これもまた推測に用いる手法となります。

* 2項ロジスティック回帰分析*▶二項分布を仮定してTRUE/FALSEの確率を予測*
* プロビット分析*▶正規分布を仮定して因子が変動した時のTRUE/FALSEの確率を予測*

どちらも判別分析の手法ですが、仮定している分布が異なる点に注意して下さい。プロビット分析は特に正規分布に基づいて限界効果が算出できる点で重宝されます。

なお、ロジスティック分析はロジスティック関数を利用するところに由来していますが、ロジスティック関数を利用するのは解析に便利（微積分が容易）な形をしているからです。このため関数の形を元に分類して*曲線回帰*と呼ぶこともあります。


## 多変数データのグループ比較
いよいよ、これまでの外側を向いた「群間の比較」と内側を向いた「因子の分析」を組み合わせて利用した手法です。

### 判別分析によるグループ分け
データ群の比較は冒頭の検定などを利用すればできることがわかっていますが、多変数のグループ比較はデータの数的な性格こそ似てるものの「やりたいこと」は全く違います。因子分析でデータの共通性からまったく新しい因子を見つけたのと同様に**データのグループ化・カテゴライズを新たに行う**（=[クラスター分析](https://www.albert2005.co.jp/knowledge/data_mining/cluster/cluster_summary)）ことが目的となります。つまり甘み、苦味、辛味がそれぞれ点数のついたメニュー表があってこれをどうもっともらしくグループ化するかというような例です。


* 幾何学的手法*▶線型判別関数やマハラノビス距離を用いてグラフを２分し、グルーピングする*
* 2項ロジスティック回帰分析・プロビット回帰分析*▶グループAに属する/属さないを確率を求め、グルーピングする*
* 分散分析*▶A群とB群の交互作用の有無を判別して、交互作用が認められるならAとBはそれぞれ別グループ*

ここのポイントは幾何学的な手法についてです。単変数の時にヒストグラムを描いて漠然とデータのイメージを掴むステップがありましたが多変数でも同様です。**２軸のグラフにプロットして当該データがどのような立ち位置にいるか俯瞰して把握する**ことは非常に役立ちます。



## ２つのデータの群が比率の場合の分析
冒頭２群の比較を行いましたが、実はやっていたことはそれぞれのデータの情報を平均という形まで削ぎ落として比較をしていただけになります。しかし２群の比較にはいわゆる二項対立のようにAとBの群の比較なども２群の比較といえます。このように２つのデータが異なる性質のデータであった場合の検討を考えてみます。


### 比率の検定
とはいえ、これまでの正規性を保つ場合の検定と扱い方は変わりません。たとえばコーヒーが好きな人とそうでない人のアンケートを取りその割合が75パーセントと25パーセントだった場合（もちろんある程度人数は多くなければこの推定自体あまりやる意味はありません）、母集団の好きな人の比率をpとおいて区間推定します。もちろん逆に母集団の比率を25パーセントとして推定しても良いですが**どう考えても後者のやり方のほうがハズす確率が高そうなことは容易に想像がつきます**。このあたりの感覚がつかめてくると論文中の推定・検定の結果を正しく読み取ること素養がつかめてきていると言えるのではないでしょうか。

### 文脈のある事象の検定
前節ではさらっと比率の検定を行っていますが、つまり**標本の割合というのは比率に置き換えられる**という基本ですが重要な部分が示されています。今流行しているベイズ統計もそうですが、現実世界の多くの事象はいきなりAかBかという議論だけではなく、その前にもっと根本的な原因があることがあります。さっきのコーヒーの話でいうともしかすると男女差や年齢層による違いがあるかもしれません。このように多くの事象は**階層になっていることがあります**。するとやるべきことは、

* フィッシャーの正確確率検定*▶本当に男女差とコーヒー好きという別々の要因は関係しているのか*
* オッズ比の検定*▶これまでの例とは逆にコーヒー好きかどうかが病気になるかどうかと関連あるかを確率の比率でとして求める。*

このように書くと実際さまざまな場合に役立ちそうに聞こえてきますが、数理的に弱い部分をついたシンプソンのパラドックスなどもあるので注意が必要です。

### （おまけ）データ群が複数ある場合。。
A、B、Cと3つ以上の項目があり且つ文脈がある場合。複雑ですね。この場合には検定ができないわけではありませんが（それぞれの文脈で起きたA、B、Cが全く同じかどうかを検定する同等性の検定や、2群の場合とと同様で独立性の検定、リジット分析など）がありますがどれも各手法で言えることが弱い気がします。というのも同等性の検定もそうですが**全く同じではない＝つまり違う**というロジックで示しているのでその手法で『言えること』が限定的です。そのためこのケースではそもそも研究手法を見直すほうが現実的とも言えます。

### （おまけ２）分散分析・実験計画法へのジャンプアップ
この節で述べた「文脈のある事象の検定」のようにAが起きてBの結果を得るというように文脈があるフレームというのは、まるで科学における実験と結果の関係に似ています。つまり何かを狙ってAという要素を変えてBという結果を得られたというのと全く同じです。それはつまり分散分析や実験計画法・タグチメソッドによる分析をやっていることにほかなりません。統計手法を論じるこのページの主旨とは違いますが、事象の「前後関係」が意識できたあとに分散分析を見るとスッと理解できる良いチャンスではないでしょうか。


## 特定のデータ形式を持つデータの分析
時系列に計測されたデータなどは特定の分析手法が存在します。

### 時系列データ
タイムシリーズに沿ったデータはその１つひとつのデータがそれぞれ前の時間の影響を引きずっている事が多く、そのメリデメを生かした分析手法を選ぶことが大切です。たとえば、

* 移動平均*▶平均化してよりデータの特徴を分かりやすくする*
* 指数平滑化*▶瞬時ノイズを低減して、次のタイムステップを予測する*
* 自己相関係数・交差相関係数・自己回帰モデル*▶タイムステップをずらしたデータ群との相関から時間遅れの影響をみる*

書籍中ではその他、カプランマイヤー法やコックス回帰分析、パネル分析（時系列の多変数分析）などが取り上げられています。

### カテゴリカル・データ
これまで多く量的データを扱ってきましたが、実際のアンケートなどではカテゴリカル・データが存在します。

* コレスポンデンス分析*▶因子分析を応用して各因子のポジショニングをプロットする*
* 多重応答分析*▶主成分分析の応用*
* コンジョイント分析*▶データに対する因子の影響度を調べる*

こうしてみるとカテゴリカル・データも今まで説明してきた分析手法をうまく応用しているだけで目新しさはありません。特にコレスポンデンス分析なんかは日本でいうと[数量化III類](https://www.wikiwand.com/ja/%E6%95%B0%E9%87%8F%E5%8C%96%E7%90%86%E8%AB%96)のことですね。



ただカテゴリカル・データの分析は**カテゴリカルに落とし込めている時点で今までやってきた分析と逆方向であることが多いです**。なぜならもともとグルーピングや潜在的な因子を見つけるために数量データを扱ってきた手法を、すでに明らかなカテゴリカル・データを数量に置き換えて当てはめているだけといっても過言ではないからです。故にマーケティングなどではこの数量化をエビデンスとして販売戦略などに役立てることもあるようですね。

## 最後に
こういったトピックではお決まりかもしれませんが、すべての手法は万能ではありません。

たとえばはずれ値（スミルノフ・グラブス検定）を見つける検定も世の中には存在しますが、それが本当にはずれ値なのかどうかは人間にしか判断できません。手法を過信しすぎることなく上手に使いたいですね。
